####################################### LSM number of levels #######################################

# We're going to do something a little simple here.  We're going to make a tree with a branching
# factor of two.  This means we'll have COLD_LEVEL_0_NUM_FILES distinct powers-of-two triangles.
LSM_LOWER_LEVEL_NUM_FILES = LSM_LEVEL_0_NUM_FILES
LSM_UPPER_LEVEL_NUM_FILES = LSM_LOWER_LEVEL_NUM_FILES * 2**ceil(LSM_NUM_LEVELS) # raw

# Cross-check:  The partition size should fit within the levels we need
assert PARTITION_SIZE <= 2**LSM_NUM_LEVELS * TARGET_FILE_SIZE * LSM_LEVEL_0_NUM_FILES
# Cross-check:  But wouldn't fit in a fewer number of partitions
assert PARTITION_SIZE >= 2**(LSM_NUM_LEVELS-1) * TARGET_FILE_SIZE * LSM_LEVEL_0_NUM_FILES

############################################ Compaction ############################################

# We ended up with some number of levels.  Now here's the key:  We'll make each level twice the
# previous level in size, and compact the lowest-n levels, where all n levels are full.  It is,
# metaphorically, the number of low order bits set.  E.g. 0b10111 would be 3 because the lowest 3
# bits are set, so it would compact to 0b11000.  Let's just do the math to figure out what this
# looks like in terms of compaction size.  The work of a compaction is O(log n) if I'm not crazy.
sums = 0
count = 0
for i in range(2**ceil(LSM_NUM_LEVELS)):
    bits = 0
    while i and (1 << bits) & i:
        bits += 1
    sums += (1 << bits) * TARGET_FILE_SIZE
    count += 1
# What falls out is the average compaction size.
COMPACTION_AVERAGE_SIZE = sums / count + LSM_LEVEL_0_NUM_FILES * TARGET_FILE_SIZE # bytes

# How many times we'll evaluate compaction daily.
COMPACTIONS_PER_DAY = ASSUMPTION1_INGEST_24H_BYTES / (LSM_LEVEL_0_NUM_FILES * TARGET_FILE_SIZE)
COMPACTIONS_PER_DAY_PER_MACHINE = COMPACTIONS_PER_DAY / MIN_MACHINES

# Now let's make sure that the available bandwidth for compaction exceeds the demand.
# These values are per-machine.
COMPACTION_BANDWIDTH_AVAILABLE_DAILY = 86400 * (MACHINE_IOPS_SEQ_WRITE * MACHINE_IOP_SIZE - ASSUMPTION1_INGEST_RATE_BYTES_S_PER_MACHINE) # bytes
COMPACTION_BANDWIDTH_DEMAND_DAILY = COMPACTION_AVERAGE_SIZE * COMPACTIONS_PER_DAY_PER_MACHINE # bytes

# We check that our demand is not within a factor of two of the limit.
assert COMPACTION_BANDWIDTH_DEMAND_DAILY < 0.5 * COMPACTION_BANDWIDTH_AVAILABLE_DAILY 

# It's important to witness what we've done here.  We can guarantee space amplification is less than
# a factor of two (the first n-1 levels are approximately the size of the nth level).  We can
# guarantee write amplification is approximately O(log n).  What are we losing?  Read amplfication.
# There are more levels, although half of them will be empty.  Because half the data resides in
# these levels, half the reads will likely come from those levels.  Bloom filters (as in LevelDB or
# RocksDB) can help.
#
# Note that scaling the cluster does not affect the compaction, as compaction is purely local.

# The structure we have also permits us to guarantee the ability to make compaction progress, if
# only locally.  Like a hot knife through layers of ice cream cake, we can carve one set of
# overlapping SSTs into two disjoint sets separated by a single key.
HOT_KNIFE_MAX_IO = 2 * LSM_NUM_LEVELS * TARGET_FILE_SIZE
