#!/usr/bin/env napkin

# This is a proof that walks through the parameters for a 16 PiB key-value store.

############################################ Parameters ############################################
# These parameters represent the assumptions and observations that will confine our work.  We'll use
# these numbers throughout as we refine the design.  These are not specific to this work, and
# capture constants that make sense as a starting point for any system.

# Unreplicated size of the cluster.
CAPACITY = 16 * Pi # bytes

# Size of each partition.  We pick a small partition size so that they can be easily re-replicated
# on failure.  This value is good for RocksDB or sst.
PARTITION_SIZE = 1024 * Gi # bytes

# Size of each file.  The target file size should be something efficient and large enough to hold
# many key-value pairs.
TARGET_FILE_SIZE = 4 * Mi # bytes

############################################# Machines #############################################

# Now let's talk about the machines we could use.  I picked the i3.metal machine as inspiration.
MACHINE_CPUS = 72
MACHINE_MEMORY = 512 * Gi # bytes
MACHINE_NUM_DISKS = 8
MACHINE_STORAGE = MACHINE_NUM_DISKS * 1900 * Gi # bytes
MACHINE_NET_RATE = 10 * 1024**3 / 8 # bytes/s
MACHINE_IOP_SIZE = 4096 # bytes
MACHINE_IOPS_RAND_READ = MACHINE_NUM_DISKS * 3.3 * M
MACHINE_IOPS_SEQ_WRITE = MACHINE_NUM_DISKS * 1.4 * M

# And how many of them we need without considering replication if we fill to 85%.
MIN_MACHINES = ceil(CAPACITY / (MACHINE_STORAGE * 0.85))

########################################### Observations ###########################################

# Let's first explore how partitioning, files, and machine counts interrelate.
PARTITIONS_PER_CLUSTER = CAPACITY / PARTITION_SIZE 
PARTITIONS_PER_MACHINE = PARTITIONS_PER_CLUSTER / MIN_MACHINES
FILES_PER_PARTITION = PARTITION_SIZE / TARGET_FILE_SIZE
FILES_PER_MACHINE = FILES_PER_PARTITION * PARTITIONS_PER_MACHINE

########################################### Assumption 1 ###########################################

# Assumption 1:  The amount of data held by the store greatly exceeds the amount of data ingested in
# any 24h period.  We'll assume approximately 1% growth per day.  This could be more or less.
#
# Start with a ratio of data ingested to data in the KVS.  This will determine the number of bytes
# ingested in any non-anomalous 24-hour period relative to cluster size.
ASSUMPTION1_INGEST_24H_RATIO = 0.10 # raw
# In bytes that's:
ASSUMPTION1_INGEST_24H_BYTES = CAPACITY * ASSUMPTION1_INGEST_24H_RATIO # bytes
# Now configure how much of the machine is in use on the ingest path.
#
# Note we're making a strong choice here to limit how much of the machine is available to write.
ASSUMPTION1_INGEST_PERCENT = 0.02 # raw

# Sanity check that assumption 1 is viable.  This is how much data we must ingest per second to keep up.
ASSUMPTION1_INGEST_RATE_BYTES_S = ASSUMPTION1_INGEST_24H_BYTES / 86400 # bytes/sec
ASSUMPTION1_INGEST_RATE_BYTES_S_PER_MACHINE = ASSUMPTION1_INGEST_RATE_BYTES_S / MIN_MACHINES # bytes/sec
assert ASSUMPTION1_INGEST_RATE_BYTES_S_PER_MACHINE < ASSUMPTION1_INGEST_PERCENT * MACHINE_IOPS_SEQ_WRITE * MACHINE_IOP_SIZE

# This is how many raw IOPS we need to perform to keep up with the ingest rate.
ASSUMPTION1_INGEST_RATE_IOPS_S = ceil(ASSUMPTION1_INGEST_RATE_BYTES_S / 4096)
ASSUMPTION1_INGEST_RATE_IOPS_S_PER_MACHINE = ASSUMPTION1_INGEST_RATE_IOPS_S / MIN_MACHINES
assert ASSUMPTION1_INGEST_RATE_IOPS_S_PER_MACHINE < ASSUMPTION1_INGEST_PERCENT * MACHINE_IOPS_SEQ_WRITE

# Check that assumption 1 is compatible with the network.
NETWORK_RESOURCES_AVAILABLE = ASSUMPTION1_INGEST_PERCENT * MACHINE_NET_RATE # bytes/sec
assert NETWORK_RESOURCES_AVAILABLE > ASSUMPTION1_INGEST_RATE_BYTES_S_PER_MACHINE

# Property:  If no assertion fails, the configured ratio is at most 2% of our ssd write rate and
# network card rate, leaving us 99% of the machine's write rate for compaction and logging.

########################################### Partitioning ###########################################

# Recall that we've created a number of partitions to manage.  
PARTITION_SIZE = PARTITION_SIZE # bytes
PARTITIONS_PER_CLUSTER = PARTITIONS_PER_CLUSTER
PARTITIONS_PER_MACHINE = PARTITIONS_PER_MACHINE
# and that we have an assumed limit on the number of bytes per second ingested into the cluster
ASSUMPTION1_INGEST_24H_BYTES = ASSUMPTION1_INGEST_24H_BYTES # bytes

# How many files do we need per partition?
PARTITION_FILES = ceil(PARTITION_SIZE / TARGET_FILE_SIZE) # raw

# How much throughput do we need to sustain in order to hit assumption 1 levels of throughput?
INGEST_PER_PARTITION_THROUGHPUT = floor(ASSUMPTION1_INGEST_24H_BYTES / 86400 / PARTITIONS_PER_CLUSTER) # bytes/sec
INGEST_PER_MACHINE_THROUGHPUT = INGEST_PER_PARTITION_THROUGHPUT * PARTITIONS_PER_MACHINE # bytes/sec

INGEST_PER_PARTITION_FILES_DAILY = ceil(INGEST_PER_PARTITION_THROUGHPUT * 86400 / TARGET_FILE_SIZE) # raw
INGEST_PER_MACHINE_FILES_DAILY = ceil(INGEST_PER_PARTITION_FILES_DAILY * PARTITIONS_PER_MACHINE)  # raw

########################################### Assumption 2 ###########################################

# Our second assumption is that we're building an LSM tree where we have a limited number of files
# in the 0'th level and a limited number of levels.
LSM_LEVEL_0_NUM_FILES = 8 # raw
LSM_NUM_LEVELS = 15 # raw

# We're going to do something a little simple here.  We're going to make a tree with a branching
# factor of two.  This means we'll have COLD_LEVEL_0_NUM_FILES distinct powers-of-two triangles.
LSM_LOWER_LEVEL_NUM_FILES = LSM_LEVEL_0_NUM_FILES
LSM_UPPER_LEVEL_NUM_FILES = LSM_LOWER_LEVEL_NUM_FILES * 2**ceil(LSM_NUM_LEVELS) # raw

# Cross-check:  The partition size should fit within the levels we need
assert PARTITION_SIZE <= 2**LSM_NUM_LEVELS * TARGET_FILE_SIZE * LSM_LEVEL_0_NUM_FILES
# Cross-check:  But wouldn't fit in a fewer number of partitions
assert PARTITION_SIZE >= 2**(LSM_NUM_LEVELS-1) * TARGET_FILE_SIZE * LSM_LEVEL_0_NUM_FILES

############################################ Compaction ############################################

# We ended up with some number of levels.  Now here's the key:  We'll make each level twice the
# previous level in size, and compact the lowest-n levels, where all n levels are full.  It is,
# metaphorically, the number of low order bits set.  E.g. 0b10111 would be 3 because the lowest 3
# bits are set, so it would compact to 0b11000.  Let's just do the math to figure out what this
# looks like in terms of compaction size.  The work of a compaction is O(log n) if I'm not crazy.
sums = 0
count = 0
for i in range(2**ceil(LSM_NUM_LEVELS)):
    bits = 0
    while i and (1 << bits) & i:
        bits += 1
    sums += (1 << bits) * TARGET_FILE_SIZE
    count += 1
# What falls out is the average compaction size.
COMPACTION_AVERAGE_SIZE = sums / count + LSM_LEVEL_0_NUM_FILES * TARGET_FILE_SIZE # bytes

# How many times we'll evaluate compaction daily.
COMPACTIONS_PER_DAY = ASSUMPTION1_INGEST_24H_BYTES / (LSM_LEVEL_0_NUM_FILES * TARGET_FILE_SIZE)
COMPACTIONS_PER_DAY_PER_MACHINE = COMPACTIONS_PER_DAY / MIN_MACHINES

# Now let's make sure that the available bandwidth for compaction exceeds the demand.
# These values are per-machine.
COMPACTION_BANDWIDTH_AVAILABLE_DAILY = 86400 * (MACHINE_IOPS_SEQ_WRITE * MACHINE_IOP_SIZE - ASSUMPTION1_INGEST_RATE_BYTES_S_PER_MACHINE) # bytes
COMPACTION_BANDWIDTH_DEMAND_DAILY = COMPACTION_AVERAGE_SIZE * COMPACTIONS_PER_DAY_PER_MACHINE 

# We check that our demand is not within a factor of two of the limit.
assert COMPACTION_BANDWIDTH_DEMAND_DAILY < 0.5 * COMPACTION_BANDWIDTH_AVAILABLE_DAILY 

# It's important to witness what we've done here.  We can guarantee space amplification is less than
# a factor of two (the first n-1 levels are approximately the size of the nth level).  We can
# guarantee write amplification is approximately O(log n).  What are we losing?  Read amplfication.
# There are more levels, although half of them will be empty.  Because half the data resides in
# these levels, half the reads will likely come from those levels.  Bloom filters (as in LevelDB or
# RocksDB) can help.
#
# Note that scaling the cluster does not affect the compaction, as compaction is purely local.
